# Finetuning Transformers Methods Showcase

This repository contains code associated with a manuscript in progress. The goal of this project is to showcase the applied linguistics research applications of an emerging method known as finetuning transformers. This technique allows researchers to train highly effective models using a relatively small amount of labeled text samples. The models can then apply labels to unlabeled text samples, frequently with an accuracy that approaches that of a trained human rater.

## Using this repository

This repository is tested with Python version 3.10. To install the dependencies, run the following command:

`pip install -r requirements.txt`
