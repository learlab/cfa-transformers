{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d0e4f70-1e5c-4404-a4c3-a5c4a34fbad9",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67822f94-b19c-49cd-a904-621d34b0ef28",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_ENTITY=langdon\n",
      "env: WANDB_PROJECT=ellipse\n",
      "env: WANDB_DIR=/home/jovyan/active-projects/ellipse-methods-showcase/bin\n"
     ]
    }
   ],
   "source": [
    "from transformers import (DataCollatorWithPadding, Trainer, TrainingArguments,\n",
    "                          LongformerTokenizer, LongformerForSequenceClassification)\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "assert torch.cuda.is_available(), 'GPU not found. You should fix this.'\n",
    "\n",
    "import wandb\n",
    "%env WANDB_ENTITY = langdon\n",
    "%env WANDB_PROJECT = ellipse\n",
    "%env WANDB_DIR = /home/jovyan/active-projects/ellipse-methods-showcase/bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb823085-da86-4620-aa3f-72ed218ba8f0",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "cb7e0d08-b5fb-4654-8c5c-fee042f96232",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_datadict(score_to_predict):\n",
    "    \n",
    "    scores = {\n",
    "        'Overall',\n",
    "        'Cohesion',\n",
    "        'Syntax',\n",
    "        'Vocabulary',\n",
    "        'Phraseology',\n",
    "        'Grammar',\n",
    "        'Conventions'\n",
    "    }\n",
    "    \n",
    "    columns_to_remove = scores.symmetric_difference([score_to_predict])\n",
    "    \n",
    "    dd = (DatasetDict\n",
    "          .load_from_disk('../data/ellipse.hf')\n",
    "          .remove_columns(columns_to_remove)\n",
    "          .rename_column(score_to_predict, 'label')\n",
    "         )\n",
    "    \n",
    "    return dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "99582168-feeb-4af1-91b7-cf70490bc256",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text_id', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 4537\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['text_id', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 972\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text_id', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 973\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_to_predict = 'Grammar'\n",
    "\n",
    "dd = get_datadict(score_to_predict)\n",
    "dd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccef9b5-5343-4d2b-8671-01f4db8221f1",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5c0c53-d196-43a8-bdff-267395e1147c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return (LongformerForSequenceClassification\n",
    "            .from_pretrained(model_name,\n",
    "                             num_labels=1)\n",
    "            .cuda()\n",
    "           )\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f81f3756-2ad0-46d9-9fe9-ed03948da315",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics_for_regression(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    mse = mean_squared_error(labels, logits)\n",
    "    rmse = mean_squared_error(labels, logits, squared=False)\n",
    "    mae = mean_absolute_error(labels, logits)\n",
    "    r2 = r2_score(labels, logits)\n",
    "\n",
    "    return {'mse': mse, 'rmse': rmse, 'mae': mae, 'r2': r2}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5713d6-5825-461e-a8f2-98a418422d26",
   "metadata": {},
   "source": [
    "## Training Paramaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9ee732-a1b3-4d74-b1ad-3c7f77b878c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-05\n",
    "batch_size = 16\n",
    "num_epochs = 4\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = '../bin',\n",
    "    optim = 'adamw_torch',\n",
    "    num_train_epochs = num_epochs,\n",
    "    per_device_train_batch_size = batch_size,\n",
    "    per_device_eval_batch_size = batch_size,\n",
    "    gradient_accumulation_steps = 4, \n",
    "    gradient_checkpointing = True,\n",
    "    weight_decay = 0.01,\n",
    "    learning_rate = learning_rate,\n",
    "    logging_dir = f'../logs/{score_to_predict}',\n",
    "    save_total_limit = 10,\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model = 'rmse',\n",
    "    evaluation_strategy = 'epoch',\n",
    "    save_strategy = 'epoch', \n",
    "    greater_is_better = False,\n",
    "    seed = SEED,\n",
    "    log_level = 'error',\n",
    "    disable_tqdm = False,\n",
    "    report_to = 'wandb',\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6a6494-4404-4df3-b4c6-a6be93f0f900",
   "metadata": {},
   "source": [
    "## Run Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383f51ca-ca0c-4031-98e4-3cd0df144e59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model_init = model_init,\n",
    "    args = training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset = dd['train'],\n",
    "    eval_dataset = dd['dev'],\n",
    "    compute_metrics = compute_metrics_for_regression,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32327ec2-e8f2-428f-9cf0-d23dfd1476b1",
   "metadata": {},
   "source": [
    "## Unused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757825cc-8432-4dea-9eb1-e151d32dc61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "model = BertModel.from_pretrained('bert-base-uncased', output_attentions=True,output_hidden_states=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=True)\n",
    "s = 'I want to sleep'\n",
    "inputs = tokenizer.encode_plus(s,return_tensors='pt', add_special_tokens=False,is_pretokenized=True)\n",
    "input_ids = inputs['input_ids']\n",
    "output = model(input_ids)\n",
    "hidden_states = output[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad5bd97a-b2d6-44f8-a33f-bc87f0877b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ./test/checkpoint-6810 were not used when initializing LongformerForSequenceClassification: ['longformer.embeddings.position_ids']\n",
      "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LongformerForSequenceClassification' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m LongformerForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./test/checkpoint-6810\u001b[39m\u001b[38;5;124m'\u001b[39m, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m preds, labs, metrics \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m(ds_t[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(scipy\u001b[38;5;241m.\u001b[39mstats\u001b[38;5;241m.\u001b[39mpearsonr(labs, preds))\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n",
      "File \u001b[0;32m~/conda_envs/wesEnv/lib/python3.10/site-packages/torch/nn/modules/module.py:1265\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1263\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1264\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1265\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1266\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LongformerForSequenceClassification' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "preds, labs, metrics = model.predict(ds_t['test'])\n",
    "print(scipy.stats.pearsonr(labs, preds))\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "preds.flatten()\n",
    "plt.scatter(preds, actual, alpha=0.5)\n",
    "plt.ylabel('true score')\n",
    "plt.xlabel('predicted score')\n",
    "plt.title('Model Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff4c8a3-da4e-4a08-bd09-ababbc6cffc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    return LongformerForSequenceClassification.from_pretrained(model_name, num_labels=1, return_dict=True).to(device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    'test', \n",
    "    evaluation_strategy='epoch', \n",
    "    disable_tqdm=True,\n",
    "    gradient_accumulation_steps=4, \n",
    "    gradient_checkpointing=True,\n",
    "    report_to='wandb',)\n",
    "\n",
    "trainer = Trainer(\n",
    "    args=training_args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=ds_t['train'],\n",
    "    eval_dataset=ds_t['valid'],\n",
    "    model_init=model_init,\n",
    "    compute_metrics=compute_metrics_for_regression,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),    \n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.hyperparameter_search(\n",
    "    direction='minimize', \n",
    "    backend='ray', \n",
    "    n_trials=10, # number of trials\n",
    "\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ellipse]",
   "language": "python",
   "name": "conda-env-ellipse-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
