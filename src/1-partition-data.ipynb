{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1571fb99-e261-4f2f-95d5-3d95068b2ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python library for working with dataframes\n",
    "import pandas as pd\n",
    "\n",
    "# Import Dataset and DatasetDict classes from the datasets library that helps us prepare our own dataset for use in training and evaulating machine learning models\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Import Python library that helps us extract certain target patterns from strings with regular expressions\n",
    "import re\n",
    "\n",
    "# Setting a seed helps us replicate results across multiple runs\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6489f7ca-8da2-4918-a3cb-4fa280e97292",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the csv file\n",
    "df = pd.read_csv('../data/All_adjudicated_ELL_data_1022.csv')\n",
    "\n",
    "# Allows us to take a quick look at the first two rows of the loaded dataframe\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c18edba7-7ab1-420d-b211-8b8f9200ec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Overall', 'Cohesion', 'Syntax', 'Vocabulary', 'Phraseology', 'Grammar', 'Conventions']\n",
    "cols1 = [col + '_1' for col in cols]\n",
    "cols2 = [col + '_2' for col in cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c9de5d-4591-4715-9531-b5da3c11955a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick and dirty word counter\n",
    "df.Text.str.split().str.len().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc5dce5-7159-40d2-bde7-f2e11584a924",
   "metadata": {},
   "source": [
    "## Build Dataset\n",
    "\n",
    "Create a DatasetDict that will hold the dataset partitions. Saving this to disk promotes reproducibility by guaranteeing that different scripts are accessing the same data splits. I find that it also helps to organize our research code.\n",
    "\n",
    "It is possible to tokenize at this stage, but I prefer to tokenize at the last minute. This affords us the flexibility of changing tokenization schemes, which could be useful if we want to test different pretrained models (that may use different tokenizers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9d5133ad-717b-45b6-ba81-aeba6dfbd8b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_datadict(df):\n",
    "    # Create list of columns that we are interested in working with\n",
    "    columns = ['ID', 'Text'] + cols1 + cols2\n",
    "    \n",
    "    # Use the above list to only select the datapoints in the columns we are interested in. \n",
    "    # We are also renaming the column 'clean_text' into 'text' since this is the only 'text' data we will be working with anyway.\n",
    "    df = df[columns].rename(columns = {col: col.lower() for col in columns})\n",
    "    \n",
    "    # Use the Dataset class method to transform a dataframe into a Dataset object.\n",
    "    ds = Dataset.from_pandas(df, preserve_index=False)\n",
    "\n",
    "    # Split data into train, development, and test sets.\n",
    "    # 70% train, 15% development, 15% test\n",
    "    # Use the SEED we defined in the first cell to ensure reproducibility of the split.\n",
    "    train_remains = ds.train_test_split(test_size=0.3, seed=SEED)\n",
    "    train = train_remains['train']\n",
    "    _remains = train_remains['test']\n",
    "    \n",
    "    dev_test = _remains.train_test_split(test_size=0.5, seed=SEED)\n",
    "    dev = dev_test['train']\n",
    "    test = dev_test['test']\n",
    "\n",
    "    dd = DatasetDict({\n",
    "        'train': train,\n",
    "        'dev': dev, \n",
    "        'test': test})\n",
    "\n",
    "    return dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "58872beb-e578-4ca9-b4ff-90970aafd855",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dd = build_datadict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30ed999c-c1ff-4b1c-9cfa-7e25a7d60083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'text', 'overall_1', 'cohesion_1', 'syntax_1', 'vocabulary_1', 'phraseology_1', 'grammar_1', 'conventions_1', 'overall_2', 'cohesion_2', 'syntax_2', 'vocabulary_2', 'phraseology_2', 'grammar_2', 'conventions_2'],\n",
       "        num_rows: 6216\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['id', 'text', 'overall_1', 'cohesion_1', 'syntax_1', 'vocabulary_1', 'phraseology_1', 'grammar_1', 'conventions_1', 'overall_2', 'cohesion_2', 'syntax_2', 'vocabulary_2', 'phraseology_2', 'grammar_2', 'conventions_2'],\n",
       "        num_rows: 1332\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'text', 'overall_1', 'cohesion_1', 'syntax_1', 'vocabulary_1', 'phraseology_1', 'grammar_1', 'conventions_1', 'overall_2', 'cohesion_2', 'syntax_2', 'vocabulary_2', 'phraseology_2', 'grammar_2', 'conventions_2'],\n",
       "        num_rows: 1332\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f77f7027-beb1-4f26-89b6-e36620175e05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a0b6d388be409f87fe2b3d1dfc6026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/6216 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1b5253cd9894ddf880466fe4f7c02b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1332 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ff8c92679b441bbaa7182ee9b27b3da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1332 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save the DatasetDict object. We will be using it in the next notebooks.\n",
    "dd.save_to_disk('../data/raw_ellipse.hf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c2fe3d-cbb1-4236-ad03-d8f2dd10de20",
   "metadata": {},
   "source": [
    "# Load dataset back in and save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5236545b-b882-4137-b20e-d0ab8b39135f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = DatasetDict.load_from_disk('../data/raw_ellipse.hf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73524e94-f8da-402e-b4ca-af0aea1d0f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([\n",
    "    dd['train'].to_pandas(),\n",
    "    dd['dev'].to_pandas(),\n",
    "    dd['test'].to_pandas()\n",
    "], names=['partition'], keys=['train', 'dev', 'test']).reset_index(level=0).reset_index(drop=True)\n",
    "df.to_csv('../data/ellipse_partitioned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05ba7a8d-5419-4cf3-9670-af6d0ac40a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from io import BytesIO\n",
    "import os\n",
    "\n",
    "# Create a BytesIO object to hold the zip file in memory\n",
    "zip_buffer = BytesIO()\n",
    "\n",
    "# Create a ZipFile object\n",
    "with zipfile.ZipFile(zip_buffer, 'a', zipfile.ZIP_DEFLATED, False) as zip_file:\n",
    "    # Iterate through each row in the DataFrame\n",
    "    for index, row in df.iterrows():\n",
    "        # Filename is e.g., train/{text_id}.txt\n",
    "        partition = 'train' if row['partition'] in ['train', 'dev'] else 'test'\n",
    "        folder_path = os.path.join(partition, f\"{row['id']}.txt\")\n",
    "\n",
    "        zip_file.writestr(folder_path, row['text'])\n",
    "\n",
    "with open('../data/ellipse_partitioned.zip', 'wb') as f:\n",
    "    f.write(zip_buffer.getvalue())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hf]",
   "language": "python",
   "name": "conda-env-hf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
