{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c083fb0-ef48-417e-b7fa-8450127f0ee9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "\n",
    "from transformers import (DataCollatorWithPadding, Trainer, TrainingArguments,\n",
    "                          LongformerTokenizer, LongformerForSequenceClassification,\n",
    "                          LongformerConfig)\n",
    "\n",
    "from transformers.models.longformer.modeling_longformer import create_position_ids_from_input_ids\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "assert torch.cuda.is_available(), 'GPU not found. You should fix this.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da158022-9578-49de-8c97-87013f68f92c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from captum.attr import Saliency, LayerIntegratedGradients, IntegratedGradients\n",
    "from captum.attr import visualization as viz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6385e094-f170-4850-8ed3-eaaa29ccac4f",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876ddc5f-338a-4aa0-ba3f-3909b4059b49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_datadict(score_to_predict):\n",
    "    \n",
    "    scores = {\n",
    "        'Overall',\n",
    "        'Cohesion',\n",
    "        'Syntax',\n",
    "        'Vocabulary',\n",
    "        'Phraseology',\n",
    "        'Grammar',\n",
    "        'Conventions'\n",
    "    }\n",
    "    \n",
    "    columns_to_remove = scores.symmetric_difference([score_to_predict])\n",
    "    \n",
    "    dd = (DatasetDict\n",
    "          .load_from_disk('../data/ellipse.hf')\n",
    "          .remove_columns(columns_to_remove)\n",
    "          .rename_column(score_to_predict, 'label')\n",
    "         )\n",
    "    \n",
    "    return dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d2116f-d160-408c-8d82-04093d013086",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "score_to_predict = 'Grammar'\n",
    "\n",
    "dd = get_datadict(score_to_predict)\n",
    "dd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d333c8fa-3d86-4389-b42b-f87195ba55c8",
   "metadata": {},
   "source": [
    "## Testbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a7fbdb-fbf8-4409-983b-07b4a3843563",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_chkpt = '../bin/checkpoint-284/'\n",
    "model = LongformerForSequenceClassification.from_pretrained(model_chkpt, num_labels=1).cuda()\n",
    "tokenizer = LongformerTokenizer.from_pretrained('allenai/longformer-base-4096')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb3b3b7-c9b2-4d6f-81db-55fad35ac1a3",
   "metadata": {},
   "source": [
    "Prepare a sample item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8e642136-d8e2-4600-bb6b-b5456518e775",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def forward_func(input_embedding, attention_mask, global_attention_mask):\n",
    "    return model(\n",
    "        inputs_embeds=input_embedding,\n",
    "        attention_mask=attention_mask,\n",
    "        global_attention_mask=global_attention_mask,\n",
    "    ).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ca96c7-760f-4958-ad79-4273c22c8ed1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(input_ids, position_ids, attention_mask):\n",
    "    return model(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        # global_attention_mask=global_attention_mask,\n",
    "        # token_type_ids=token_type_ids,\n",
    "        position_ids=position_ids,\n",
    "    ).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d454c071-deea-4a52-85e5-318b6c1e0293",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def summarize_attributions(attributions):\n",
    "    attributions = attributions.sum(dim=-1).squeeze(0)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "    return attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5567f237-bb12-4546-9611-8fb7f383159a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5\n",
      "['<s>', 'I', 'Believe', 'people', 'who', 'have', 'positive', 'attitude', \"'s\", 'towards', 'life', ',', 'are', 'one', 'of', 'the', 'biggest', 'keys', 'to', 'success', 'in', 'life', '.', 'You', 'will']\n",
      "LongformerSequenceClassifierOutput(loss=None, logits=tensor([[3.5902]], device='cuda:0'), hidden_states=None, attentions=None, global_attentions=None)\n",
      "tensor([[3.5470]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "sample = dd['train'].shuffle()[0]\n",
    "text_id = sample.pop('text_id')\n",
    "true_score = sample.pop('label')    \n",
    "print(true_score)\n",
    "\n",
    "sample = {k: torch.tensor(v).unsqueeze(0).cuda() for k, v in sample.items()}\n",
    "\n",
    "manual_embed = model.longformer.embeddings(\n",
    "    input_ids=sample['input_ids'],\n",
    ")\n",
    "\n",
    "\n",
    "sample['global_attention_mask'] = torch.zeros_like(sample['input_ids'])\n",
    "sample['global_attention_mask'][:, 0] = 1\n",
    "\n",
    "tokens = [\n",
    "    t.replace('Ġ', '')\n",
    "    for t in \n",
    "    tokenizer.convert_ids_to_tokens(\n",
    "        sample['input_ids'][0].detach().tolist()\n",
    "    )\n",
    "]\n",
    "\n",
    "print(tokens[:25])\n",
    "\n",
    "with torch.no_grad():    \n",
    "    print(model(sample['input_ids']))\n",
    "    \n",
    "    print(\n",
    "        forward_func(\n",
    "            manual_embed,\n",
    "            attention_mask=sample['attention_mask'],\n",
    "            global_attention_mask=sample['global_attention_mask'],\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f89b71a8-b1a8-401d-97f5-8e00198e9b2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "saliency = Saliency(forward_func)\n",
    "\n",
    "attribution = saliency.attribute(inputs=manual_embed,\n",
    "                                 additional_forward_args=(\n",
    "                                     sample['attention_mask'],\n",
    "                                     sample['global_attention_mask'],\n",
    "                                 ),\n",
    "                                 abs=False\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "33ee2b86-0ae7-49c2-901e-82c9811d4241",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pred_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m attribution_sum \u001b[38;5;241m=\u001b[39m summarize_attributions(attribution)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# storing couple samples in an array for visualization purposes\u001b[39;00m\n\u001b[1;32m      4\u001b[0m position_vis \u001b[38;5;241m=\u001b[39m viz\u001b[38;5;241m.\u001b[39mVisualizationDataRecord(\n\u001b[1;32m      5\u001b[0m     attribution_sum, \u001b[38;5;66;03m# token attributions\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m     torch\u001b[38;5;241m.\u001b[39mmax(torch\u001b[38;5;241m.\u001b[39msoftmax(\u001b[43mpred_score\u001b[49m[\u001b[38;5;241m0\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)), \u001b[38;5;66;03m# pred_prob\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mround\u001b[39m(pred_score\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39mitem(), \u001b[38;5;241m2\u001b[39m), \u001b[38;5;66;03m# pred_class\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     true_score, \u001b[38;5;66;03m# true_class\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;66;03m# attr_class\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     attribution_sum\u001b[38;5;241m.\u001b[39msum(), \u001b[38;5;66;03m# attr_score\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     tokens, \u001b[38;5;66;03m# raw_input_ids\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;66;03m# convergence score\u001b[39;00m\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[1m\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVisualizations\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[0m\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     16\u001b[0m viz\u001b[38;5;241m.\u001b[39mvisualize_text([position_vis])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pred_score' is not defined"
     ]
    }
   ],
   "source": [
    "attribution_sum = summarize_attributions(attribution)\n",
    "\n",
    "# storing couple samples in an array for visualization purposes\n",
    "position_vis = viz.VisualizationDataRecord(\n",
    "    attribution_sum, # token attributions\n",
    "    torch.max(torch.softmax(pred_score[0], dim=0)), # pred_prob\n",
    "    round(pred_score.logits.item(), 2), # pred_class\n",
    "    true_score, # true_class\n",
    "    None, # attr_class\n",
    "    attribution_sum.sum(), # attr_score\n",
    "    tokens, # raw_input_ids\n",
    "    None # convergence score\n",
    ")\n",
    "\n",
    "print('\\033[1m', 'Visualizations', '\\033[0m')\n",
    "viz.visualize_text([position_vis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84325a0e-2f1d-447b-95d7-fde7b1ec5533",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "ds = dd.with_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
    "\n",
    "dataloader = DataLoader(ds['dev'].shuffle().select(range(10)), batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49c9fcf-0322-405d-baea-0a03941e3e2d",
   "metadata": {},
   "source": [
    "### Saliency with Interpretable Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4fcfed2d-8e1f-4c77-b8e0-7df73bfaf713",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (478) must match the size of tensor b (768) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m input_emb \u001b[38;5;241m=\u001b[39m interpretable_emb\u001b[38;5;241m.\u001b[39mindices_to_embeddings(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 15\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mposition_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m saliency \u001b[38;5;241m=\u001b[39m Saliency(predict)\n\u001b[1;32m     19\u001b[0m attribution, delta \u001b[38;5;241m=\u001b[39m saliency\u001b[38;5;241m.\u001b[39mattribute(inputs\u001b[38;5;241m=\u001b[39minput_emb,\n\u001b[1;32m     20\u001b[0m                                         additional_forward_args\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m     21\u001b[0m                                             batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mposition_ids\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     22\u001b[0m                                             batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m     23\u001b[0m                                         \u001b[38;5;66;03m# return_convergence_delta=True\u001b[39;00m\n\u001b[1;32m     24\u001b[0m                                        )\n",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(input_ids, position_ids, attention_mask)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(input_ids, position_ids, attention_mask):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# global_attention_mask=global_attention_mask,\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# token_type_ids=token_type_ids,\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlogits\n",
      "File \u001b[0;32m~/conda_envs/ellipse/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/conda_envs/ellipse/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py:1940\u001b[0m, in \u001b[0;36mLongformerForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, global_attention_mask, head_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1937\u001b[0m     \u001b[38;5;66;03m# global attention on cls token\u001b[39;00m\n\u001b[1;32m   1938\u001b[0m     global_attention_mask[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1940\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlongformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1941\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mglobal_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1944\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1945\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1946\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1947\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1948\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1949\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1950\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1951\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1952\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1953\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[0;32m~/conda_envs/ellipse/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/conda_envs/ellipse/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py:1729\u001b[0m, in \u001b[0;36mLongformerModel.forward\u001b[0;34m(self, input_ids, attention_mask, global_attention_mask, head_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1727\u001b[0m \u001b[38;5;66;03m# merge `global_attention_mask` and `attention_mask`\u001b[39;00m\n\u001b[1;32m   1728\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1729\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_to_attention_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_attention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1731\u001b[0m padding_len, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pad_to_window_size(\n\u001b[1;32m   1732\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1733\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     pad_token_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id,\n\u001b[1;32m   1738\u001b[0m )\n\u001b[1;32m   1740\u001b[0m \u001b[38;5;66;03m# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\u001b[39;00m\n\u001b[1;32m   1741\u001b[0m \u001b[38;5;66;03m# ourselves in which case we just need to make it broadcastable to all heads.\u001b[39;00m\n",
      "File \u001b[0;32m~/conda_envs/ellipse/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py:1644\u001b[0m, in \u001b[0;36mLongformerModel._merge_to_attention_mask\u001b[0;34m(self, attention_mask, global_attention_mask)\u001b[0m\n\u001b[1;32m   1639\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_merge_to_attention_mask\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_mask: torch\u001b[38;5;241m.\u001b[39mTensor, global_attention_mask: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m   1640\u001b[0m     \u001b[38;5;66;03m# longformer self attention expects attention mask to have 0 (no attn), 1 (local attn), 2 (global attn)\u001b[39;00m\n\u001b[1;32m   1641\u001b[0m     \u001b[38;5;66;03m# (global_attention_mask + 1) => 1 for local attention, 2 for global attention\u001b[39;00m\n\u001b[1;32m   1642\u001b[0m     \u001b[38;5;66;03m# => final attention_mask => 0 for no attention, 1 for local attention 2 for global attention\u001b[39;00m\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1644\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m \u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mglobal_attention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1645\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1646\u001b[0m         \u001b[38;5;66;03m# simply use `global_attention_mask` as `attention_mask`\u001b[39;00m\n\u001b[1;32m   1647\u001b[0m         \u001b[38;5;66;03m# if no `attention_mask` is given\u001b[39;00m\n\u001b[1;32m   1648\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m global_attention_mask \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (478) must match the size of tensor b (768) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    labels = batch.pop('label')\n",
    "            \n",
    "    batch['position_ids'] = create_position_ids_from_input_ids(batch['input_ids'], model.config.pad_token_id)\n",
    "    \n",
    "    batch = {k: v.cuda() for k, v in batch.items()}\n",
    "\n",
    "    tokens = tokenizer.convert_ids_to_tokens(batch['input_ids'][0].detach().tolist())\n",
    "    \n",
    "    tokens = [t.replace('Ġ', '') for t in tokens]\n",
    "    \n",
    "    input_emb = interpretable_emb.indices_to_embeddings(batch['input_ids'])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        score = predict(input_emb)\n",
    "        \n",
    "    saliency = Saliency(predict)\n",
    "    \n",
    "    attribution, delta = saliency.attribute(inputs=input_emb,\n",
    "                                            additional_forward_args=(\n",
    "                                                batch['position_ids'],\n",
    "                                                batch['attention_mask']),\n",
    "                                            # return_convergence_delta=True\n",
    "                                           )\n",
    "    \n",
    "    attribution_sum = summarize_attributions(attribution)\n",
    "    \n",
    "    \n",
    "    # storing couple samples in an array for visualization purposes\n",
    "    position_vis = viz.VisualizationDataRecord(\n",
    "        attribution_sum,\n",
    "        torch.max(torch.softmax(score[0], dim=0)),\n",
    "        score,\n",
    "        score,\n",
    "        str(0),\n",
    "        attribution_sum.sum(),       \n",
    "        tokens,\n",
    "        delta)\n",
    "\n",
    "    print('\\033[1m', 'Visualizations', '\\033[0m')\n",
    "    viz.visualize_text([position_vis])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf4eb74-5f4d-4579-81b0-a81d1b692e6f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Layer Integrated Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24e20d8-3152-4efb-b1d5-4199624e3391",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    labels = batch.pop('label')\n",
    "        \n",
    "    # outputs = model(**batch, output_hidden_states=True)\n",
    "    \n",
    "    batch['position_ids'] = create_position_ids_from_input_ids(batch['input_ids'], model.config.pad_token_id)\n",
    "    \n",
    "    batch = {k: v.cuda() for k, v in batch.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        score = model(**batch).logits\n",
    "    \n",
    "    lig = LayerIntegratedGradients(predict, model.longformer.embeddings)\n",
    "    \n",
    "    tokens = tokenizer.convert_ids_to_tokens(batch['input_ids'][0].detach().tolist())\n",
    "    \n",
    "    tokens = [t.replace('Ġ', '') for t in tokens]\n",
    "    \n",
    "    attribution, delta = lig.attribute(inputs=batch['input_ids'],\n",
    "                                       additional_forward_args=(\n",
    "                                                                batch['position_ids'],\n",
    "                                                                batch['attention_mask']),\n",
    "                                       return_convergence_delta=True)\n",
    "    \n",
    "    attribution_sum = summarize_attributions(attribution)\n",
    "    \n",
    "    \n",
    "    # storing couple samples in an array for visualization purposes\n",
    "    position_vis = viz.VisualizationDataRecord(\n",
    "        attribution_sum,\n",
    "        torch.max(torch.softmax(score[0], dim=0)),\n",
    "        score,\n",
    "        score,\n",
    "        str(0),\n",
    "        attribution_sum.sum(),       \n",
    "        tokens,\n",
    "        delta)\n",
    "\n",
    "    print('\\033[1m', 'Visualizations', '\\033[0m')\n",
    "    viz.visualize_text([position_vis])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642b0e99-c070-4387-80b2-56a8a1f69bbf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Code from Kaggle\n",
    "\n",
    "https://www.kaggle.com/code/rhtsingh/interpreting-text-models-with-bert-on-tpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077e8dba-f0cc-4a68-a11a-aa4b8289bd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(text, label):\n",
    "    input_ids, ref_input_ids, sep_id = construct_input_ref_pair(text, ref_token_id, sep_token_id, cls_token_id)\n",
    "    token_type_ids, ref_token_type_ids = construct_input_ref_token_type_pair(input_ids, sep_id)\n",
    "    position_ids, ref_position_ids = construct_input_ref_pos_id_pair(input_ids)\n",
    "    attention_mask = construct_attention_mask(input_ids)\n",
    "\n",
    "    indices = input_ids[0].detach().tolist()\n",
    "    all_tokens = tokenizer.convert_ids_to_tokens(indices)\n",
    "    \n",
    "    if label == 0:\n",
    "        lig = LayerIntegratedGradients(custom_forward_0, model.bert.embeddings)\n",
    "    elif label == 1:\n",
    "        lig = LayerIntegratedGradients(custom_forward_1, model.bert.embeddings)\n",
    "    elif label == 2:\n",
    "        lig = LayerIntegratedGradients(custom_forward_2, model.bert.embeddings)\n",
    "    \n",
    "    attributions_main, delta_main = lig.attribute(inputs=input_ids,\n",
    "                                                  baselines=ref_input_ids,\n",
    "                                                  n_steps = 150,\n",
    "                                                  additional_forward_args=(token_type_ids, attention_mask),\n",
    "                                                  return_convergence_delta=True)\n",
    "    \n",
    "    score = predict(input_ids, token_type_ids, attention_mask)\n",
    "    attributions_main = attributions_main.cpu()\n",
    "    delta_main = delta_main.cpu()\n",
    "    score = score.cpu()\n",
    "    add_attributions_to_visualizer(attributions_main, delta_main, text, score, label, all_tokens)\n",
    "    \n",
    "def add_attributions_to_visualizer(attributions, delta, text, score, label, all_tokens):\n",
    "    attributions = attributions.sum(dim=-1).squeeze(0)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "    attributions = attributions.cpu()\n",
    "\n",
    "    score_vis.append(\n",
    "        viz.VisualizationDataRecord(\n",
    "            attributions,\n",
    "            torch.softmax(score, dim = 1)[0][label],\n",
    "            torch.argmax(torch.softmax(score, dim = 1)[0]),\n",
    "            label,\n",
    "            text,\n",
    "            attributions.sum(),\n",
    "            all_tokens,\n",
    "            delta\n",
    "        )\n",
    "    ) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ellipse]",
   "language": "python",
   "name": "conda-env-ellipse-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
