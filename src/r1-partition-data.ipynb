{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1571fb99-e261-4f2f-95d5-3d95068b2ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python library for working with dataframes\n",
    "import pandas as pd\n",
    "\n",
    "# Import Dataset and DatasetDict classes from the datasets library that helps us prepare our own dataset for use in training and evaulating machine learning models\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "# Import Python library that helps us extract certain target patterns from strings with regular expressions\n",
    "import re\n",
    "\n",
    "# Setting a seed helps us replicate results across multiple runs\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6489f7ca-8da2-4918-a3cb-4fa280e97292",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Rater_1</th>\n",
       "      <th>Overall_1</th>\n",
       "      <th>Cohesion_1</th>\n",
       "      <th>Syntax_1</th>\n",
       "      <th>Vocabulary_1</th>\n",
       "      <th>Phraseology_1</th>\n",
       "      <th>Grammar_1</th>\n",
       "      <th>...</th>\n",
       "      <th>Identifying_Info_1</th>\n",
       "      <th>Rater_2</th>\n",
       "      <th>Overall_2</th>\n",
       "      <th>Cohesion_2</th>\n",
       "      <th>Syntax_2</th>\n",
       "      <th>Vocabulary_2</th>\n",
       "      <th>Phraseology_2</th>\n",
       "      <th>Grammar_2</th>\n",
       "      <th>Conventions_2</th>\n",
       "      <th>Identifying_Info_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021000071.txt</td>\n",
       "      <td>2021000071</td>\n",
       "      <td>To the Principal,\\r\\n\\r\\nI think that policy 1...</td>\n",
       "      <td>hannah-page</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>alorapruitt</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021000501.txt</td>\n",
       "      <td>2021000501</td>\n",
       "      <td>Dear, TEACHER_NAME\\r\\n\\r\\nI think phone policy...</td>\n",
       "      <td>hannah-page</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>alorapruitt</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Filename          ID  \\\n",
       "0  2021000071.txt  2021000071   \n",
       "1  2021000501.txt  2021000501   \n",
       "\n",
       "                                                Text      Rater_1  Overall_1  \\\n",
       "0  To the Principal,\\r\\n\\r\\nI think that policy 1...  hannah-page          3   \n",
       "1  Dear, TEACHER_NAME\\r\\n\\r\\nI think phone policy...  hannah-page          3   \n",
       "\n",
       "   Cohesion_1  Syntax_1  Vocabulary_1  Phraseology_1  Grammar_1  ...  \\\n",
       "0           3         4             4              4          3  ...   \n",
       "1           3         2             3              3          3  ...   \n",
       "\n",
       "   Identifying_Info_1      Rater_2 Overall_2  Cohesion_2  Syntax_2  \\\n",
       "0                   0  alorapruitt         3           4         3   \n",
       "1                   0  alorapruitt         3           4         3   \n",
       "\n",
       "   Vocabulary_2  Phraseology_2  Grammar_2  Conventions_2  Identifying_Info_2  \n",
       "0             3              3          3              3                   0  \n",
       "1             4              3          4              3                   0  \n",
       "\n",
       "[2 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the csv file\n",
    "df = pd.read_csv('../data/All_adjudicated_ELL_data_1022.csv')\n",
    "\n",
    "# Allows us to take a quick look at the first two rows of the loaded dataframe\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c18edba7-7ab1-420d-b211-8b8f9200ec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Overall', 'Cohesion', 'Syntax', 'Vocabulary', 'Phraseology', 'Grammar', 'Conventions']\n",
    "cols1 = [col + '_1' for col in cols]\n",
    "cols2 = [col + '_2' for col in cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c542cd-6124-4ae5-9298-c7f804e5fb7b",
   "metadata": {},
   "source": [
    "## Partitions by Specific Raters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26494b57-a498-4f3e-8cf4-2333fca628d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all rows where {rater_name} worked as a rater. Return new pd.DataFrame with their judgments on new columns.\n",
    "def get_per_rater(rater_name):\n",
    "    out1 = df[df[\"Rater_1\"].str.match(rater_name)].assign(\n",
    "        Overall = lambda x: x.Overall_1,\n",
    "        Cohesion = lambda x: x.Cohesion_1,\n",
    "        Syntax = lambda x: x.Syntax_1,\n",
    "        Vocabulary = lambda x: x.Vocabulary_1,\n",
    "        Phraseology = lambda x: x.Phraseology_1,\n",
    "        Grammar = lambda x: x.Grammar_1,\n",
    "        Conventions = lambda x: x.Conventions_1\n",
    "    )\n",
    "    out2 = df[df[\"Rater_2\"].str.match(rater_name)].assign(\n",
    "        Overall = lambda x: x.Overall_2,\n",
    "        Cohesion = lambda x: x.Cohesion_2,\n",
    "        Syntax = lambda x: x.Syntax_2,\n",
    "        Vocabulary = lambda x: x.Vocabulary_2,\n",
    "        Phraseology = lambda x: x.Phraseology_2,\n",
    "        Grammar = lambda x: x.Grammar_2,\n",
    "        Conventions = lambda x: x.Conventions_2\n",
    "    )\n",
    "    return pd.concat([out1, out2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "beda7ba3-81ed-44af-823a-8d5c158f243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ap = get_per_rater(\"alorapruitt\")\n",
    "train_bb = get_per_rater(\"brittnybyrom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "821663f2-2e45-4725-9011-b4f7736484be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_per_rater_pair(name1, name2):\n",
    "    rater_name_pattern = \"|\".join([name1, name2])\n",
    "    temp_df = df[df[\"Rater_1\"].str.match(rater_name_pattern) & df[\"Rater_2\"].str.match(rater_name_pattern)]\n",
    "    out_df = temp_df.copy()\n",
    "    name1_is_rater2 = out_df[\"Rater_2\"].str.match(name2)\n",
    "    name2_is_rater1 = out_df[\"Rater_1\"].str.match(name1)\n",
    "    out_df.loc[ name1_is_rater2, cols1 ] = temp_df[cols2].rename(columns={k: v for k,v in zip(cols2, cols1)})\n",
    "    out_df.loc[ name2_is_rater1, cols2 ] = temp_df[cols1].rename(columns={k: v for k,v in zip(cols1, cols2)})\n",
    "    return out_df\n",
    "\n",
    "test = get_per_rater_pair('jbarton8', 'sulynnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6b52b988-333a-4211-a362-29683640b49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables `ap` and `bb` are probably `train_ap` and `train_bb` here? Changing the variable names. \n",
    "train_idx = set(df.index) - set(train_ap.index) - set(train_bb.index) - set(test.index)\n",
    "train1 = df.loc[list(train_idx)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc5dce5-7159-40d2-bde7-f2e11584a924",
   "metadata": {},
   "source": [
    "## Build Dataset\n",
    "\n",
    "Create a DatasetDict that will hold the dataset partitions. Saving this to disk promotes reproducibility by guaranteeing that different scripts are accessing the same data splits. I find that it also helps to organize our research code.\n",
    "\n",
    "It is possible to tokenize at this stage, but I prefer to tokenize at the last minute. This affords us the flexibility of changing tokenization schemes, which could be useful if we want to test different pretrained models (that may use different tokenizers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9d5133ad-717b-45b6-ba81-aeba6dfbd8b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_datadict(df):\n",
    "    # Create list of columns that we are interested in working with\n",
    "    columns = ['ID', 'Text'] + cols1 + cols2\n",
    "    \n",
    "    # Use the above list to only select the datapoints in the columns we are interested in. \n",
    "    # We are also renaming the column 'clean_text' into 'text' since this is the only 'text' data we will be working with anyway.\n",
    "    df = df[columns].rename(columns = {col: col.lower() for col in columns})\n",
    "    \n",
    "    # Use the Dataset class method to transform a dataframe into a Dataset object.\n",
    "    ds = Dataset.from_pandas(df, preserve_index=False)\n",
    "\n",
    "    # Split data into train, development, and test sets.\n",
    "    # 70% train, 15% development, 15% test\n",
    "    # Use the SEED we defined in the first cell to ensure reproducibility of the split.\n",
    "    train_remains = ds.train_test_split(test_size=0.3, seed=SEED)\n",
    "    train = train_remains['train']\n",
    "    _remains = train_remains['test']\n",
    "    \n",
    "    dev_test = _remains.train_test_split(test_size=0.5, seed=SEED)\n",
    "    dev = dev_test['train']\n",
    "    test = dev_test['test']\n",
    "\n",
    "    dd = DatasetDict({\n",
    "        'train': train,\n",
    "        'dev': dev, \n",
    "        'test': test})\n",
    "\n",
    "    return dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "58872beb-e578-4ca9-b4ff-90970aafd855",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dd = build_datadict(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30ed999c-c1ff-4b1c-9cfa-7e25a7d60083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'text', 'overall_1', 'cohesion_1', 'syntax_1', 'vocabulary_1', 'phraseology_1', 'grammar_1', 'conventions_1', 'overall_2', 'cohesion_2', 'syntax_2', 'vocabulary_2', 'phraseology_2', 'grammar_2', 'conventions_2'],\n",
       "        num_rows: 6216\n",
       "    })\n",
       "    dev: Dataset({\n",
       "        features: ['id', 'text', 'overall_1', 'cohesion_1', 'syntax_1', 'vocabulary_1', 'phraseology_1', 'grammar_1', 'conventions_1', 'overall_2', 'cohesion_2', 'syntax_2', 'vocabulary_2', 'phraseology_2', 'grammar_2', 'conventions_2'],\n",
       "        num_rows: 1332\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'text', 'overall_1', 'cohesion_1', 'syntax_1', 'vocabulary_1', 'phraseology_1', 'grammar_1', 'conventions_1', 'overall_2', 'cohesion_2', 'syntax_2', 'vocabulary_2', 'phraseology_2', 'grammar_2', 'conventions_2'],\n",
       "        num_rows: 1332\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f77f7027-beb1-4f26-89b6-e36620175e05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a0b6d388be409f87fe2b3d1dfc6026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/6216 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1b5253cd9894ddf880466fe4f7c02b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1332 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ff8c92679b441bbaa7182ee9b27b3da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1332 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save the DatasetDict object. We will be using it in the next notebooks.\n",
    "dd.save_to_disk('../data/raw_ellipse.hf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hf]",
   "language": "python",
   "name": "conda-env-hf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
