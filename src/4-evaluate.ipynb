{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "027fa894-691c-4fcd-a08d-a6243e96ce37",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluation\n",
    "\n",
    "In this notebook, we will be evaluating our model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb313011-39e8-4644-bba1-bf50fee00bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Classes for tokenization and model training\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "# tqdm is a progress bar that visualizes the training progress\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Import DatasetDict which will help us prepare our own dataset for use in training and evaulating machine learning models\n",
    "from datasets import DatasetDict\n",
    "\n",
    "# Import library that helps us work with arrays\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Import functions for model evaluation\n",
    "from sklearn.metrics import mean_squared_error, cohen_kappa_score, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c25179e-8a45-407d-b5a4-21471fc9356b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the DatasetDict object we created in the previous notebook. \n",
    "datadict = DatasetDict.load_from_disk('../data/ellipse.hf/')\n",
    "\n",
    "# We are specifically interested in using the test set since we are in our model evaluation phase\n",
    "ds = datadict['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49779b2a-ec4d-48c6-a38b-19f3b0f84321",
   "metadata": {},
   "source": [
    "## Retrieve both human ratings for these scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "058f5e7f-3e29-4fae-8e44-381cb4a4a237",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/both_raters.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "db83b5f9-ecc3-484d-b1f1-fd7acbf051a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6248c30454f4ebd9dc33683643a828b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/973 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def add_rater_scores(example):\n",
    "    row = df.loc[df.text_id_original == example['text_id']]\n",
    "    for score_name in ['Vocabulary_1', 'Vocabulary_2', 'Grammar_1', 'Grammar_2']:\n",
    "        example[score_name] = row[score_name].iloc[0]\n",
    "    return example\n",
    "\n",
    "ds = ds.map(add_rater_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c9b642ef-da3b-4aa1-b702-16722af94160",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function will compare the model's predictions to the actual labels, evaluate the model's performance using two different metrics,\n",
    "# and print out the results.\n",
    "def evaluate_performance(dataset, predictions, score_to_predict):\n",
    "    labels = dataset[score_to_predict]\n",
    "    results_dict = {\n",
    "        'Mean Squared error': mean_squared_error(labels, predictions),\n",
    "        'R-squared': r2_score(labels, predictions),\n",
    "        f'QWK_{score_to_predict}_1': cohen_kappa_score(dataset[f'{score_to_predict}_1'], np.round(predictions), weights='quadratic'),\n",
    "        f'QWK_{score_to_predict}_2': cohen_kappa_score(dataset[f'{score_to_predict}_2'], np.round(predictions), weights='quadratic'),\n",
    "    }\n",
    "    display(pd.DataFrame.from_dict(results_dict, orient='index'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6e005a8b-933b-4539-bb09-8e20fc364e6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model inference pipeline that uses our finetuned model\n",
    "def predict(dataset, score_to_predict):\n",
    "    \n",
    "    pipe = pipeline('text-classification',\n",
    "                    model=f'../bin/{score_to_predict.lower()}-model/',\n",
    "                    truncation=True,\n",
    "                    function_to_apply='none',\n",
    "                   )\n",
    "    \n",
    "    predictions = [pipe(text)[0]['score'] for text in tqdm(dataset['text'])]\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a43f31e1-232d-45a1-b4f2-7d71c8a4e385",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mean Squared error</th>\n",
       "      <td>0.261034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-squared</th>\n",
       "      <td>0.476316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QWK_Grammar_1</th>\n",
       "      <td>0.536748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QWK_Grammar_2</th>\n",
       "      <td>0.528642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           0\n",
       "Mean Squared error  0.261034\n",
       "R-squared           0.476316\n",
       "QWK_Grammar_1       0.536748\n",
       "QWK_Grammar_2       0.528642"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run model inference for the grammar prediction model\n",
    "grammar_predictions = predict(ds, 'grammar')\n",
    "\n",
    "# Evaluate model performance\n",
    "evaluate_performance(ds, grammar_predictions, 'Grammar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "77de9d5e-9002-484b-bc4e-e80e8c950c7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dab44221e18478697251c27e3edf1ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/973 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mean Squared error</th>\n",
       "      <td>0.194913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-squared</th>\n",
       "      <td>0.466889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QWK_Vocabulary_1</th>\n",
       "      <td>0.507475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QWK_Vocabulary_2</th>\n",
       "      <td>0.498717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           0\n",
       "Mean Squared error  0.194913\n",
       "R-squared           0.466889\n",
       "QWK_Vocabulary_1    0.507475\n",
       "QWK_Vocabulary_2    0.498717"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Do the same for the vocabulary model\n",
    "vocabulary_predictions = predict(ds, 'vocabulary')\n",
    "evaluate_performance(ds, vocabulary_predictions, 'Vocabulary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07672e9-a543-4a90-864d-88f8e069b0da",
   "metadata": {},
   "source": [
    "### Check Vocabulary Model Predicting Grammar Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c01144e6-db2b-457a-ab00-800ff38905a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mean Squared error</th>\n",
       "      <td>0.421544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-squared</th>\n",
       "      <td>0.154302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QWK_Grammar_1</th>\n",
       "      <td>0.407878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QWK_Grammar_2</th>\n",
       "      <td>0.385815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           0\n",
       "Mean Squared error  0.421544\n",
       "R-squared           0.154302\n",
       "QWK_Grammar_1       0.407878\n",
       "QWK_Grammar_2       0.385815"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_performance(ds, vocabulary_predictions, 'Grammar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61920b62-7a70-4c50-a5e5-ba86fae4cc63",
   "metadata": {},
   "source": [
    "### Check Grammar Model Predicting Vocabulary Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5d185c26-1d52-4e24-a94c-a93466381f46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mean Squared error</th>\n",
       "      <td>0.268450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>R-squared</th>\n",
       "      <td>0.265759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QWK_Vocabulary_1</th>\n",
       "      <td>0.476646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QWK_Vocabulary_2</th>\n",
       "      <td>0.490952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           0\n",
       "Mean Squared error  0.268450\n",
       "R-squared           0.265759\n",
       "QWK_Vocabulary_1    0.476646\n",
       "QWK_Vocabulary_2    0.490952"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate_performance(ds, grammar_predictions, 'Vocabulary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765ba499-845d-4e0a-ac93-45a7b4353244",
   "metadata": {},
   "source": [
    "## Drilling down on Truncation\n",
    "\n",
    "Truncation has a small but predictable effect on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ca763a75-04bf-4b74-9585-621fd9a9aa0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (685 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def get_num_tokens(sample):\n",
    "    input_ids = tokenizer(sample['text'], truncation=False)['input_ids']\n",
    "    return len(input_ids)\n",
    "\n",
    "# this is a list of boolean values that indicates whether each sample would be truncated.\n",
    "num_tokens = np.array([get_num_tokens(sample) for sample in ds])\n",
    "truncated = num_tokens > 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a9a13287-aacc-411c-a757-fe85111fc77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean truncated tokens 202.9025641025641\n",
      "Std truncated tokens 184.212894516768\n",
      "Count NON truncated: 583\n",
      "Count truncated: 390\n",
      "Count Total: 973\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean truncated tokens\", (num_tokens[truncated] - 512).mean())\n",
    "print(\"Std truncated tokens\", (num_tokens[truncated] - 512).std())\n",
    "print(\"Count NON truncated:\", np.array(ds['Grammar'])[~truncated].shape[0])\n",
    "print(\"Count truncated:\", np.array(ds['Grammar'])[truncated].shape[0])\n",
    "print(\"Count Total:\", len(ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39d4ce7-a47c-4516-8084-59ffdcedabf6",
   "metadata": {},
   "source": [
    "### Grammar\n",
    "The quadratic weighted kappa between human raters for grammar score was 0.593, so the model's QWK of 0.53 on truncated texts is likely to be sufficient. As mentioned elsewhere, methods exist to overcome model max length if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ff09c2a-ff7f-4b37-838a-e05f929ec687",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on samples that were truncated:\n",
      "Mean squared error: 0.26809612334568805\n",
      "Quadratic Weighted Kappa: 0.5381294964028777\n",
      "R-squared: 0.4966201128802773\n"
     ]
    }
   ],
   "source": [
    "print('Performance on samples that were truncated:')\n",
    "evaluate_performance(np.array(ds['Grammar'])[truncated], np.array(grammar_predictions)[truncated])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a8ec5ee-3f25-49af-af06-dc267fad709e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on samples that were NOT truncated:\n",
      "Mean squared error: 0.25631007898899016\n",
      "Quadratic Weighted Kappa: 0.5731561102648518\n",
      "R-squared: 0.45623259198870936\n"
     ]
    }
   ],
   "source": [
    "print('Performance on samples that were NOT truncated:')\n",
    "evaluate_performance(np.array(ds['Grammar'])[~truncated], np.array(grammar_predictions)[~truncated])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d62b2c-d74a-48a2-ad47-2d8073f0d3a3",
   "metadata": {},
   "source": [
    "### Vocabulary\n",
    "The quadratic weighted kappa between human raters for Vocabulary score was 0.518. The same pattern holds for the vocabulary score predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fc837cc-c5cc-4ff4-92c5-0777d3105842",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on samples that were truncated:\n",
      "Mean squared error: 0.22931245660332122\n",
      "Quadratic Weighted Kappa: 0.4999765467423425\n",
      "R-squared: 0.4137316263989821\n"
     ]
    }
   ],
   "source": [
    "print('Performance on samples that were truncated:')\n",
    "evaluate_performance(np.array(ds['Vocabulary'])[truncated], np.array(vocabulary_predictions)[truncated])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "503f14ad-e4fd-4c2c-af53-6e01586ad1a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on samples that were NOT truncated:\n",
      "Mean squared error: 0.1719019895065474\n",
      "Quadratic Weighted Kappa: 0.5297352790203124\n",
      "R-squared: 0.4501760176219026\n"
     ]
    }
   ],
   "source": [
    "print('Performance on samples that were NOT truncated:')\n",
    "evaluate_performance(np.array(ds['Vocabulary'])[~truncated], np.array(vocabulary_predictions)[~truncated])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1453d6b1-5f76-4421-aa53-a4a7d9fd565f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar\n",
      "Truncated: 0.5325920447074294\n",
      "Not Truncated: 0.4713597674534922\n",
      "Vocabulary\n",
      "Truncated: 0.3911390532544379\n",
      "Not Truncated: 0.31264912956877094\n"
     ]
    }
   ],
   "source": [
    "def describe(metric):\n",
    "    print(metric)\n",
    "    print(\"Truncated:\", np.array(ds[metric])[truncated].var())\n",
    "    print(\"Not Truncated:\", np.array(ds[metric])[~truncated].var())\n",
    "\n",
    "describe('Grammar')\n",
    "describe('Vocabulary')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:trf]",
   "language": "python",
   "name": "conda-env-trf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
