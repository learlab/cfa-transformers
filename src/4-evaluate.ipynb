{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "027fa894-691c-4fcd-a08d-a6243e96ce37",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluation\n",
    "\n",
    "In this notebook, we will be evaluating our model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb313011-39e8-4644-bba1-bf50fee00bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Classes for tokenization and model training\n",
    "from transformers import AutoTokenizer, TextClassificationPipeline\n",
    "\n",
    "# tqdm is a progress bar that visualizes the training progress\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Import DatasetDict which will help us prepare our own dataset for use in training and evaulating machinelearning models\n",
    "from datasets import DatasetDict\n",
    "\n",
    "# Import library that helps us work with arrays\n",
    "import numpy as np\n",
    "\n",
    "# Import functions for model evaluation\n",
    "from sklearn.metrics import mean_squared_error, cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c25179e-8a45-407d-b5a4-21471fc9356b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the DatasetDict object we created in the previous notebook. \n",
    "datadict = DatasetDict.load_from_disk('../data/ellipse.hf/')\n",
    "\n",
    "# We are specifically interested in using the test set since we are in our model evaluation phase\n",
    "ds = datadict['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c9b642ef-da3b-4aa1-b702-16722af94160",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This function will compare the model's predictions to the actual labels, evaluate the model's performance using two different metrics,\n",
    "# and print out the results.\n",
    "def evaluate_performance(labels, predictions):\n",
    "    mse = mean_squared_error(labels, predictions)\n",
    "    qwk = cohen_kappa_score(np.round(labels),\n",
    "                            np.round(predictions),\n",
    "                            weights='quadratic')\n",
    "    \n",
    "    print('Mean squared error:', mse)\n",
    "    print('Quadratic Weighted Kappa:', qwk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6e005a8b-933b-4539-bb09-8e20fc364e6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model inference pipeline that uses our finetuned model\n",
    "def predict(dataset, score_to_predict):\n",
    "    \n",
    "    pipe = pipeline('text-classification',\n",
    "                    model=f'../bin/{score_to_predict.lower()}-model/',\n",
    "                    truncation=True,\n",
    "                    function_to_apply='none',\n",
    "                   )\n",
    "    \n",
    "    labels = dataset\n",
    "    predictions = [pipe(text)[0]['score'] for text in tqdm(dataset['text'])]\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2f38c2f6-05b9-4a39-ad75-46f2eb1cfcbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6ae6ac08e66409d80147b6936fc47c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/973 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.26103419570908415\n",
      "Cohen's Kappa: 0.5627542019572562\n"
     ]
    }
   ],
   "source": [
    "# Run model inference for the grammar prediction model\n",
    "grammar_predictions = predict(ds, 'grammar')\n",
    "\n",
    "# Evaluate model performance\n",
    "evaluate_performance(ds['Grammar'], grammar_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "77de9d5e-9002-484b-bc4e-e80e8c950c7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "996c49fe8fc94bd8a98a274774d047df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/973 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.19491337478093973\n",
      "Quadratic Weighted Kappa: 0.552087599588894\n"
     ]
    }
   ],
   "source": [
    "# Do the same for the vocabulary model\n",
    "vocabulary_predictions = predict(ds, 'vocabulary')\n",
    "evaluate_performance(ds['Vocabulary'], vocabulary_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765ba499-845d-4e0a-ac93-45a7b4353244",
   "metadata": {},
   "source": [
    "## Drilling down on Truncation\n",
    "\n",
    "Truncation has a small but predictable effect on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ca763a75-04bf-4b74-9585-621fd9a9aa0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (685 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def would_be_truncated(sample):\n",
    "    input_ids = tokenizer(sample['text'], truncation=False)['input_ids']\n",
    "    return True if len(input_ids) > 512 else False\n",
    "\n",
    "# this is a list of boolean values that indicates whether each sample would be truncated.\n",
    "truncated = np.array([would_be_truncated(sample) for sample in ds])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39d4ce7-a47c-4516-8084-59ffdcedabf6",
   "metadata": {},
   "source": [
    "### Grammar\n",
    "The quadratic weighted kappa between human raters for grammar score was 0.593, so the model's QWK of 0.53 on truncated texts is likely to be sufficient. As mentioned elsewhere, methods exist to overcome model max length if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5ff09c2a-ff7f-4b37-838a-e05f929ec687",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on samples that were truncated:\n",
      "Mean squared error: 0.2680961383174671\n",
      "Quadratic Weighted Kappa: 0.5381294964028777\n"
     ]
    }
   ],
   "source": [
    "print('Performance on samples that were truncated:')\n",
    "evaluate_performance(np.array(ds['Grammar'])[truncated], np.array(grammar_predictions)[truncated])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3a8ec5ee-3f25-49af-af06-dc267fad709e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on samples that were NOT truncated:\n",
      "Mean squared error: 0.2563100831580218\n",
      "Quadratic Weighted Kappa: 0.5731561102648518\n"
     ]
    }
   ],
   "source": [
    "print('Performance on samples that were NOT truncated:')\n",
    "evaluate_performance(np.array(ds['Grammar'])[~truncated], np.array(grammar_predictions)[~truncated])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d62b2c-d74a-48a2-ad47-2d8073f0d3a3",
   "metadata": {},
   "source": [
    "### Vocabulary\n",
    "The quadratic weighted kappa between human raters for Vocabulary score was 0.518. The same pattern holds for the vocabulary score predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2fc837cc-c5cc-4ff4-92c5-0777d3105842",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on samples that were truncated:\n",
      "Mean squared error: 0.22931245016904137\n",
      "Quadratic Weighted Kappa: 0.4999765467423425\n"
     ]
    }
   ],
   "source": [
    "print('Performance on samples that were truncated:')\n",
    "evaluate_performance(np.array(ds['Vocabulary'])[truncated], np.array(vocabulary_predictions)[truncated])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "503f14ad-e4fd-4c2c-af53-6e01586ad1a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on samples that were NOT truncated:\n",
      "Mean squared error: 0.17190198644241547\n",
      "Quadratic Weighted Kappa: 0.5297352790203124\n"
     ]
    }
   ],
   "source": [
    "print('Performance on samples that were NOT truncated:')\n",
    "evaluate_performance(np.array(ds['Vocabulary'])[~truncated], np.array(vocabulary_predictions)[~truncated])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
